{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# initialPlanningAndTests.ipynb\n",
    "## Marcus Viscardi,    February 13, 2024\n",
    "\n",
    "\n",
    "## From JA:\n",
    "* (i) figure or subpanel containing poly(A) tail information, and reproducibility of measurements at various read count cutoffs\n",
    "* (ii) supp figure or subpanel reproducibility of abundance (avg count, or SD/avg) at various read count cutoffs, to justify the counts we chose\n",
    "* (iii) supp figure or subpanel  with “saturation analysis”, showing how many genes we contain reads for.\n",
    "\n",
    "## Planning:\n",
    "### Target (i): Tail information and reproducibility\n",
    "I have done this analysis at some point. I think it predated the poly(A) paper?! I will need to find the code where I decided on the cutoffs for reads.\n",
    "3 plots:\n",
    "1. Standards: violin with 10, 15, and 60 bp tails\n",
    "    - I produced these recently for Josh, just polish them and pop them in!\n",
    "2. Subsampled tails: x-axis = number_of_test_samples, y-axis = mean_tail_length (and SEM)\n",
    "    - This will be a plot for each standard.\n",
    "3. Reproducibility of tail uniqueness (for CDFs we use):\n",
    "    - CDF for subsets of reads per standard (eg. 5, 10, 25, 50)\n",
    "    - Perform each subsetting 100 times or so, this will give us a bunch of CDFs\n",
    "    - Then take the middle 95% of the CDFs and the mean of the middle 95% of the CDFs for each tail length\n",
    "    - Plot the mean CDF and the 95% +/- CDFs for each tail standard (10, 15, & 60)\n",
    "    - This will give us a good way to show that we can differentiate between tails and that we can do so reproducibly!\n",
    "### Target (ii): Reproducibility of abundance\n",
    "X-axis = RPM/read counts for per gene\n",
    "Y-axis = SD/avg RPM/read counts per gene\n",
    "\n",
    "This could be an overall scatter, or instead take the average of the SD/avg for each read_count bin and plot that as a line. This should show that at some point the data doesn't keep getting \"better\" with more depth\n",
    "\n",
    "Another way to target the same idea is just a rocket plot with a vertical and horizontal cutoff showing what we identified as out cutoff!\n",
    "\n",
    "\n",
    "### Target (iii): Saturation analysis\n",
    "I did this for the Poly(A) paper in [subsamplingReadsVsProteinCoding.ipynb](../polyA_manuscriptPostReviewScripts/subsamplingReadsVsProteinCoding.ipynb). I just adjusted some inputs and ran it again... Worked great!\n",
    "\n",
    "Josh thinks we could additionally have a plot with:\n",
    "X-axis = reads_per_gene cutoff (1 to 200?)\n",
    "Y-axis = cumulative number of genes above X cutoff"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a97353910babb96"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic: 2024-04-18 12:06:31 | > 'Imports done!'\n"
     ]
    }
   ],
   "source": [
    "import nanoporePipelineCommon as npCommon\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sea\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from icecream import ic\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.stats import mannwhitneyu, ks_2samp\n",
    "\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def __time_formatter__():\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return f\"ic: {now} | > \"\n",
    "ic.configureOutput(prefix=__time_formatter__)\n",
    "\n",
    "_ = ic(\"Imports done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T19:06:31.786442645Z",
     "start_time": "2024-04-18T19:06:31.489457486Z"
    }
   },
   "id": "cbf404dc3cce2e0c",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-18T19:08:17.370573866Z",
     "start_time": "2024-04-18T19:06:31.745301741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading oldN2... Done!\n",
      "Loading 240219_mergedOnReads.parquet... Done. Loaded 701,680 rows.\n",
      "\n",
      "Loading newerN2... Done!\n",
      "Loading 230327_mergedOnReads.plusStandards.parquet... Done. Loaded 1,053,185 rows.\n",
      "\n",
      "Loading newerS6... Done!\n",
      "Loading 230403_mergedOnReads.plusStandards.parquet... Done. Loaded 1,226,144 rows.\n",
      "\n",
      "Loading newerS5... Done!\n",
      "Loading 230410_mergedOnReads.plusStandards.parquet... Done. Loaded 557,991 rows.\n",
      "\n",
      "Loading thirdN2... Done!\n",
      "Loading 230920_mergedOnReads.plusStandards.parquet... Done. Loaded 1,186,602 rows.\n",
      "\n",
      "Loading thirdS5... Done!\n",
      "Loading 230920_mergedOnReads.plusStandards.parquet... Done. Loaded 1,440,373 rows.\n",
      "\n",
      "Loading thirdS6... Done!\n",
      "Loading 230918_mergedOnReads.plusStandards.parquet... Done. Loaded 560,899 rows.\n"
     ]
    }
   ],
   "source": [
    "obj_dict = {}\n",
    "libs_to_run = [\n",
    "    \"oldN2\",\n",
    "    # \"oldS6\",\n",
    "    \"newerN2\",\n",
    "    \"newerS6\",\n",
    "    \"newerS5\",\n",
    "    \"thirdN2\",\n",
    "    \"thirdS5\",\n",
    "    \"thirdS6\",\n",
    "]\n",
    "for lib in libs_to_run:\n",
    "    print(f\"\\nLoading {lib}...\", end=\"\")\n",
    "    obj = npCommon.NanoporeRun(run_nickname=lib)\n",
    "    obj_dict[lib] = obj\n",
    "    print(\" Done!\")\n",
    "    obj.load_mergedOnReads()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Just pick one library to work with for now. We could merge all libs if we wanted to, but I think that would be overkill!\n",
    "TEST_LIB = \"thirdN2\"\n",
    "\n",
    "def plot_cdf(tester_df__, ax__=None, sample_size=25, repeats=1000,\n",
    "             stds_color_map={'10': 'red', '15': 'green', '60': 'blue'},\n",
    "             x_axis_lim=200, log_x=False):\n",
    "    if ax__ is None:\n",
    "        fig, ax__ = plt.subplots(figsize=(7, 5))\n",
    "    \n",
    "    for std, color in stds_color_map.items():\n",
    "        std_df = tester_df__.query(\"assignment == @std\")\n",
    "\n",
    "        x_values = []\n",
    "        y_values = []\n",
    "\n",
    "        # Define the bin edges to span the range of 'polya_length_floored' across all subsets\n",
    "        bin_edges = np.linspace(std_df['polya_length_floored'].min(), std_df['polya_length_floored'].max(), num=10_000)\n",
    "\n",
    "        for _ in range(repeats):\n",
    "            try:\n",
    "                sample_df = std_df.sample(sample_size)\n",
    "            except ValueError as e:\n",
    "                if sample_size > std_df.shape[0]:\n",
    "                    print(f\"Sample size too large for std {std}! To resolve this we will sample with replacement!\")\n",
    "                    sample_df = std_df.sample(sample_size, replace=True)\n",
    "                else:\n",
    "                    raise e\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred with std {std} and sample size {sample_size}!\")\n",
    "                raise e\n",
    "            # Use the defined bin edges to calculate the histogram\n",
    "            hist, _ = np.histogram(sample_df['polya_length_floored'], bins=bin_edges)\n",
    "            # Calculate the cumulative sum to get the CDF\n",
    "            cdf = np.cumsum(hist)\n",
    "            # Normalize the CDF by the total count\n",
    "            cdf = cdf / sample_df.shape[0]\n",
    "            x_values.append(bin_edges[:-1])  # Exclude the last bin edge because np.histogram returns one more bin edge than the number of bins\n",
    "            y_values.append(cdf)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        x_values = np.array(x_values)\n",
    "        y_values = np.array(y_values)\n",
    "\n",
    "        # Calculate mean and 95% confidence interval of y values\n",
    "        mean_y = np.mean(y_values, axis=0)\n",
    "        lower_bound, upper_bound = np.percentile(y_values, [2.5, 97.5], axis=0)\n",
    "\n",
    "        # The x values should be the same for all repeats, so just take the x values from the first repeat\n",
    "        assert (x_values[0] == x_values[1]).all()\n",
    "        x = x_values[0]\n",
    "        ax__.plot(x, mean_y, color=color, label=f\"Mean CDF - {std}A\")\n",
    "        ax__.fill_between(x, lower_bound, upper_bound, color=color, alpha=0.1, label=f\"95% CI - {std}A\")\n",
    "        ax__.set_title(f\"Mean CDF and 95% CI for {repeats} repeats of {sample_size} reads\")\n",
    "    ax__.set_xlabel(\"Tail length\")\n",
    "    ax__.set_ylabel(\"Proportion of reads\")\n",
    "    ax__.legend()\n",
    "    \n",
    "    if log_x:\n",
    "        ax__.set_xlim(5, x_axis_lim)\n",
    "        ax__.set_xscale('log')\n",
    "        ax__.grid(True, which=\"both\", alpha=0.5)\n",
    "    else:\n",
    "        ax__.set_xlim(0, x_axis_lim)\n",
    "        ax__.grid(True, which=\"both\", alpha=0.5)\n",
    "    \n",
    "    if ax__ is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return ax__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T19:08:17.382007233Z",
     "start_time": "2024-04-18T19:08:17.380129493Z"
    }
   },
   "id": "53b7953b00fe89ae",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <u>Plots for Poly(A) Standards and Tail Lengths</u> (Target I)\n",
    "\n",
    "## First, data loading:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3e61874c8e0e514"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing newerN2...\n",
      "assignment\n",
      "60    68724\n",
      "15    65995\n",
      "10    57541\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing newerS6...\n",
      "assignment\n",
      "60    3802\n",
      "15    3638\n",
      "10    3453\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing newerS5...\n",
      "assignment\n",
      "60    2739\n",
      "15    2448\n",
      "10    2160\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing thirdN2...\n",
      "assignment\n",
      "60    9411\n",
      "15    9100\n",
      "10    7617\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing thirdS5...\n",
      "assignment\n",
      "60    10685\n",
      "15    10084\n",
      "10     8615\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing thirdS6...\n",
      "assignment\n",
      "60    4417\n",
      "15    4041\n",
      "10    3343\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target_standards = [\"10\", \"15\", \"60\"]\n",
    "standards_df_dict = {}\n",
    "for lib, obj in obj_dict.items():\n",
    "    if lib == 'oldN2':\n",
    "        continue\n",
    "    print(f\"\\nProcessing {lib}...\")\n",
    "    df = obj.mergedOnReads_df.copy()\n",
    "    df = df.query(\"assignment in @target_standards & qc_tag_polya == 'PASS'\")\n",
    "    df = df[[\"read_id\", \"assignment\", \"polya_length\"]]\n",
    "    df['polya_length_floored'] = df['polya_length'].apply(np.round).astype(int)\n",
    "    print(df.value_counts('assignment'))\n",
    "    standards_df_dict[lib] = df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T19:08:19.787581668Z",
     "start_time": "2024-04-18T19:08:17.382860011Z"
    }
   },
   "id": "80c963f5a3c2b2d4",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bootstrapping analysis of how many reads are needed to get reproducible tail lengths\n",
    "\n",
    "### Initial data processing:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "173e7f03c62cff4d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Standard 10 - bootstrapping:   0%|          | 0/245 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e18e4b95b31d40f6a514ab3178ee998b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Standard 15 - bootstrapping:   0%|          | 0/245 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d228d60983e4818a9451b9f0e0298c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Standard 60 - bootstrapping:   0%|          | 0/245 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "807815167f0543d39190d597e0b0bc26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_lib = TEST_LIB\n",
    "\n",
    "SUBSET_SIZES = range(5, 250, 1)\n",
    "RUNS_PER_SUBSET = 200\n",
    "\n",
    "subset_tails_super_dict = {}\n",
    "subset_means_super_dict = {}\n",
    "subset_sems_super_dict = {}\n",
    "\n",
    "for std in target_standards:\n",
    "    std_df = standards_df_dict[test_lib].query(\"assignment == @std\")\n",
    "    subset_tails_dict = {}\n",
    "    subset_means_dict = {}\n",
    "    subset_sems_dict = {}\n",
    "    iterator = tqdm(SUBSET_SIZES, desc=f\"Standard {std} - bootstrapping\")\n",
    "    for size in iterator:\n",
    "        subset_tails = []\n",
    "        subset_means = []\n",
    "        subset_sems = []\n",
    "        for run_i in range(RUNS_PER_SUBSET):\n",
    "            if run_i % 10 == 0:\n",
    "                iterator.set_postfix_str(f\"Size: {size} - Run: {run_i}\")\n",
    "            subset = std_df.sample(size)\n",
    "            subset_means.append(subset['polya_length'].mean())\n",
    "            subset_sems.append(subset['polya_length'].sem())\n",
    "            subset_tails.append(subset['polya_length'].values)\n",
    "        subset_tails_dict[size] = subset_tails\n",
    "        subset_means_dict[size] = np.mean(subset_means)\n",
    "        subset_sems_dict[size] = np.mean(subset_sems)\n",
    "    subset_tails_super_dict[std] = subset_tails_dict\n",
    "    subset_means_super_dict[std] = subset_means_dict\n",
    "    subset_sems_super_dict[std] = subset_sems_dict\n",
    "    \n",
    "    x = np.array(SUBSET_SIZES)\n",
    "    y = np.array([subset_means_dict[size] for size in SUBSET_SIZES])\n",
    "    sem = np.array([subset_sems_dict[size] for size in SUBSET_SIZES])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T19:09:05.201282526Z",
     "start_time": "2024-04-18T19:08:19.790824362Z"
    }
   },
   "id": "3a1cab87388cb5fa",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mann-Whitney U tests for reproducibility of differentiating tail groups"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36c50caa2ae06acb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Mann-Whitney U tests:   0%|          | 0/245 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "109e5d02b82a44d9ba00cfa211917fd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: ks_2samp: Exact calculation unsuccessful. Switching to method=asymp.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     20\u001B[0m     iterator\u001B[38;5;241m.\u001B[39mset_postfix_str(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize10\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - Run: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrun_i\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m _, p_val \u001B[38;5;241m=\u001B[39m \u001B[43mmannwhitneyu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtails_dict_10\u001B[49m\u001B[43m[\u001B[49m\u001B[43mrun_i\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtails_dict_15\u001B[49m\u001B[43m[\u001B[49m\u001B[43mrun_i\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m mwu_results_10v15_long[(size10, run_i)] \u001B[38;5;241m=\u001B[39m p_val \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m p_val_cutoff\n\u001B[1;32m     24\u001B[0m _, p_val \u001B[38;5;241m=\u001B[39m mannwhitneyu(tails_dict_15[run_i], tails_dict_60[run_i])\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531\u001B[0m, in \u001B[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    529\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sentinel:\n\u001B[1;32m    530\u001B[0m     samples \u001B[38;5;241m=\u001B[39m _remove_sentinel(samples, paired, sentinel)\n\u001B[0;32m--> 531\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mhypotest_fun_out\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    532\u001B[0m res \u001B[38;5;241m=\u001B[39m result_to_tuple(res)\n\u001B[1;32m    533\u001B[0m res \u001B[38;5;241m=\u001B[39m _add_reduced_axes(res, reduced_axes, keepdims)\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/scipy/stats/_mannwhitneyu.py:493\u001B[0m, in \u001B[0;36mmannwhitneyu\u001B[0;34m(x, y, use_continuity, alternative, axis, method)\u001B[0m\n\u001B[1;32m    491\u001B[0m     p \u001B[38;5;241m=\u001B[39m _mwu_state\u001B[38;5;241m.\u001B[39msf(U\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m), n1, n2)\n\u001B[1;32m    492\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masymptotic\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 493\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[43m_get_mwu_z\u001B[49m\u001B[43m(\u001B[49m\u001B[43mU\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mranks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontinuity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_continuity\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    494\u001B[0m     p \u001B[38;5;241m=\u001B[39m stats\u001B[38;5;241m.\u001B[39mnorm\u001B[38;5;241m.\u001B[39msf(z)\n\u001B[1;32m    495\u001B[0m p \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m f\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/scipy/stats/_mannwhitneyu.py:174\u001B[0m, in \u001B[0;36m_get_mwu_z\u001B[0;34m(U, n1, n2, ranks, axis, continuity)\u001B[0m\n\u001B[1;32m    171\u001B[0m n \u001B[38;5;241m=\u001B[39m n1 \u001B[38;5;241m+\u001B[39m n2\n\u001B[1;32m    173\u001B[0m \u001B[38;5;66;03m# Tie correction according to [2]\u001B[39;00m\n\u001B[0;32m--> 174\u001B[0m tie_term \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_along_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_tie_term\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mranks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    175\u001B[0m s \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msqrt(n1\u001B[38;5;241m*\u001B[39mn2\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m12\u001B[39m \u001B[38;5;241m*\u001B[39m ((n \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m-\u001B[39m tie_term\u001B[38;5;241m/\u001B[39m(n\u001B[38;5;241m*\u001B[39m(n\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))))\n\u001B[1;32m    177\u001B[0m \u001B[38;5;66;03m# equivalent to using scipy.stats.tiecorrect\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;66;03m# T = np.apply_along_axis(stats.tiecorrect, -1, ranks)\u001B[39;00m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;66;03m# s = np.sqrt(T * n1 * n2 * (n1+n2+1) / 12.0)\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/numpy/lib/shape_base.py:379\u001B[0m, in \u001B[0;36mapply_along_axis\u001B[0;34m(func1d, axis, arr, *args, **kwargs)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    377\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    378\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 379\u001B[0m res \u001B[38;5;241m=\u001B[39m asanyarray(\u001B[43mfunc1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43minarr_view\u001B[49m\u001B[43m[\u001B[49m\u001B[43mind0\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    381\u001B[0m \u001B[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001B[39;00m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001B[39;00m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;66;03m# laid out so that each write is contiguous.\u001B[39;00m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001B[39;00m\n\u001B[1;32m    385\u001B[0m buff \u001B[38;5;241m=\u001B[39m zeros(inarr_view\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m res\u001B[38;5;241m.\u001B[39mshape, res\u001B[38;5;241m.\u001B[39mdtype)\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/scipy/stats/_mannwhitneyu.py:163\u001B[0m, in \u001B[0;36m_tie_term\u001B[0;34m(ranks)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Tie correction term\"\"\"\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;66;03m# element i of t is the number of elements sharing rank i\u001B[39;00m\n\u001B[0;32m--> 163\u001B[0m _, t \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mranks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (t\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m3\u001B[39m \u001B[38;5;241m-\u001B[39m t)\u001B[38;5;241m.\u001B[39msum(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/numpy/lib/arraysetops.py:317\u001B[0m, in \u001B[0;36munique\u001B[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001B[0m\n\u001B[1;32m    314\u001B[0m     uniq \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmoveaxis(uniq, \u001B[38;5;241m0\u001B[39m, axis)\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m uniq\n\u001B[0;32m--> 317\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43m_unique1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconsolidated\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mequal_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mequal_nan\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m output \u001B[38;5;241m=\u001B[39m (reshape_uniq(output[\u001B[38;5;241m0\u001B[39m]),) \u001B[38;5;241m+\u001B[39m output[\u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _unpack_tuple(output)\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/numpy/lib/arraysetops.py:336\u001B[0m, in \u001B[0;36m_unique1d\u001B[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001B[0m\n\u001B[1;32m    334\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar[perm]\n\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 336\u001B[0m     ar\u001B[38;5;241m.\u001B[39msort()\n\u001B[1;32m    337\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar\n\u001B[1;32m    338\u001B[0m mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(aux\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mbool_)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "p_val_cutoff = 0.05\n",
    "\n",
    "mwu_results_10v15_long = {}\n",
    "mwu_results_15v60_long = {}\n",
    "ks_results_10v15_long = {}\n",
    "ks_results_15v60_long = {}\n",
    "# subset_tails_super_dict = std: {size: [run1, run2, ...]}\n",
    "assert subset_tails_super_dict['10'].keys() == subset_tails_super_dict['15'].keys() == subset_tails_super_dict['60'].keys()\n",
    "iterator = tqdm(zip(subset_tails_super_dict['10'].items(),\n",
    "                    subset_tails_super_dict['15'].items(),\n",
    "                    subset_tails_super_dict['60'].items()),\n",
    "                total=len(subset_tails_super_dict['10']),\n",
    "                desc=\"Mann-Whitney U tests\")\n",
    "for ((size10, tails_dict_10),\n",
    "     (size15, tails_dict_15),\n",
    "     (size60, tails_dict_60)) in iterator:\n",
    "    assert size10 == size15 == size60\n",
    "    for run_i in range(RUNS_PER_SUBSET):\n",
    "        if run_i % 10 == 0:\n",
    "            iterator.set_postfix_str(f\"Size: {size10} - Run: {run_i}\")\n",
    "        _, p_val = mannwhitneyu(tails_dict_10[run_i], tails_dict_15[run_i])\n",
    "        mwu_results_10v15_long[(size10, run_i)] = p_val <= p_val_cutoff\n",
    "        \n",
    "        _, p_val = mannwhitneyu(tails_dict_15[run_i], tails_dict_60[run_i])\n",
    "        mwu_results_15v60_long[(size10, run_i)] = p_val <= p_val_cutoff\n",
    "        \n",
    "        _, p_val = ks_2samp(tails_dict_10[run_i], tails_dict_15[run_i])\n",
    "        ks_results_10v15_long[(size10, run_i)] = p_val <= p_val_cutoff\n",
    "        \n",
    "        _, p_val = ks_2samp(tails_dict_15[run_i], tails_dict_60[run_i])\n",
    "        ks_results_15v60_long[(size10, run_i)] = p_val <= p_val_cutoff\n",
    "\n",
    "mwu_results_10v15 = {size: sum([mwu_results_10v15_long[(size, run_i)] / RUNS_PER_SUBSET for run_i in range(RUNS_PER_SUBSET)]) for size in SUBSET_SIZES}\n",
    "mwu_results_15v60 = {size: sum([mwu_results_15v60_long[(size, run_i)] / RUNS_PER_SUBSET for run_i in range(RUNS_PER_SUBSET)]) for size in SUBSET_SIZES}\n",
    "ks_results_10v15 = {size: sum([ks_results_10v15_long[(size, run_i)] / RUNS_PER_SUBSET for run_i in range(RUNS_PER_SUBSET)]) for size in SUBSET_SIZES}\n",
    "ks_results_15v60 = {size: sum([ks_results_15v60_long[(size, run_i)] / RUNS_PER_SUBSET for run_i in range(RUNS_PER_SUBSET)]) for size in SUBSET_SIZES}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T19:10:24.678900044Z",
     "start_time": "2024-04-18T19:09:05.201233175Z"
    }
   },
   "id": "bb7899f817391061",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting the results of bootstrapping and MWU tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e712bf5c9b334056"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 12))\n",
    "\n",
    "# Define colors for each standard\n",
    "colors = {'10': 'red', '15': 'green', '60': 'blue'}\n",
    "\n",
    "# Create the top subplot that spans across columns\n",
    "# ax1 = plt.subplot2grid((3, 2), (0, 0), colspan=2)\n",
    "ax0 = plt.subplot2grid((3, 2), (0, 0), colspan=1)\n",
    "ax1 = plt.subplot2grid((3, 2), (0, 1), colspan=1)\n",
    "\n",
    "# Create the bottom left and right subplots\n",
    "ax2 = plt.subplot2grid((3, 2), (1, 0))\n",
    "ax3 = plt.subplot2grid((3, 2), (1, 1))\n",
    "\n",
    "# add two more plots for the KS tests below:\n",
    "ax4 = plt.subplot2grid((3, 2), (2, 0))\n",
    "ax5 = plt.subplot2grid((3, 2), (2, 1))\n",
    "\n",
    "# Now you can use ax1 for the top plot, and ax2 and ax3 for the bottom plots\n",
    "plot_cdf(standards_df_dict[test_lib], ax__=ax0, log_x=True, sample_size=50, repeats=200)\n",
    "# sea.ecdfplot(data=standards_df_dict[test_lib], x='polya_length',\n",
    "#              hue='assignment', ax=ax0, palette=colors)\n",
    "# ax0.set_title(\"CDF of tail lengths for different standards\")\n",
    "# ax0.set_xlabel(\"Tail length\")\n",
    "# ax0.set_ylabel(\"Proportion of reads\")\n",
    "# ax0.set_xlim(5, 250)\n",
    "# ax0.set_xscale('log')\n",
    "# ax0.grid(True, which=\"both\", alpha=0.5)\n",
    "\n",
    "\n",
    "# Top plot with all the standards\n",
    "for std in target_standards:\n",
    "    x = np.array(SUBSET_SIZES)\n",
    "    y = np.array([subset_means_super_dict[std][size] for size in SUBSET_SIZES])\n",
    "    sem = np.array([subset_sems_super_dict[std][size] for size in SUBSET_SIZES])\n",
    "    ax1.plot(x, y, label=f\"Standard {std}\", color=colors[std])\n",
    "    ax1.fill_between(x, y-sem, y+sem, alpha=0.2, color=colors[std])\n",
    "    ax1.axhline(int(std), color=colors[std], linestyle='--')\n",
    "\n",
    "ax1.set_title(\"Tail length reproducibility for all standards \\n(bootstrapped 200 times per point,\\nwith sample sizes from 5 to 250)\")\n",
    "ax1.set_xlabel(\"Number of reads sampled\")\n",
    "ax1.set_ylabel(\"Mean tail length\")\n",
    "ax1.legend()\n",
    "\n",
    "# Middle left plot with 10 vs 15 MWU\n",
    "sea.lineplot(x=SUBSET_SIZES, y=[mwu_results_10v15[size] for size in SUBSET_SIZES], ax=ax2)\n",
    "ax2.set_title(f\"Reproducibility of MWU test for 10 vs 15\\n(w/ {RUNS_PER_SUBSET} runs per point)\")\n",
    "ax2.set_xlabel(\"Number of reads sampled\")\n",
    "ax2.set_ylabel(\"Proportion of tests with p-value < 0.05\")\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "# Middle right plot with 15 vs 60 MWU\n",
    "sea.lineplot(x=SUBSET_SIZES, y=[mwu_results_15v60[size] for size in SUBSET_SIZES], ax=ax3)\n",
    "ax3.set_title(f\"Reproducibility of MWU test for 15 vs 60\\n(w/ {RUNS_PER_SUBSET} runs per point)\")\n",
    "ax3.set_xlabel(\"Number of reads sampled\")\n",
    "ax3.set_ylabel(\"Proportion of tests with p-value < 0.05\")\n",
    "ax3.set_ylim(0, 1.05)\n",
    "\n",
    "# Bottom left plot with 10 vs 15 KS\n",
    "sea.lineplot(x=SUBSET_SIZES, y=[ks_results_10v15[size] for size in SUBSET_SIZES], ax=ax4)\n",
    "ax4.set_title(f\"Reproducibility of KS test for 10 vs 15\\n(w/ {RUNS_PER_SUBSET} runs per point)\")\n",
    "ax4.set_xlabel(\"Number of reads sampled\")\n",
    "ax4.set_ylabel(\"Proportion of tests with p-value < 0.05\")\n",
    "ax4.set_ylim(0, 1.05)\n",
    "\n",
    "# Bottom right plot with 15 vs 60 KS\n",
    "sea.lineplot(x=SUBSET_SIZES, y=[ks_results_15v60[size] for size in SUBSET_SIZES], ax=ax5)\n",
    "ax5.set_title(f\"Reproducibility of KS test for 15 vs 60\\n(w/ {RUNS_PER_SUBSET} runs per point)\")\n",
    "ax5.set_xlabel(\"Number of reads sampled\")\n",
    "ax5.set_ylabel(\"Proportion of tests with p-value < 0.05\")\n",
    "ax5.set_ylim(0, 1.05)\n",
    "\n",
    "# Final plot adjustments\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/tailLengthReproducibility.{test_lib}.{RUNS_PER_SUBSET}perPoint.allInOne.plusKS.plusCDF.png\")\n",
    "plt.savefig(f\"figures/tailLengthReproducibility.{test_lib}.{RUNS_PER_SUBSET}perPoint.allInOne.plusKS.plusCDF.svg\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.667292080Z"
    }
   },
   "id": "2fa6f8a9dcde9818",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 95% CI for CDFs of tail lengths as another method to show ability to differentiate tails populations with different numbers of observations\n",
    "\n",
    "### Initial plotting of CDFs for each standard and each subset size:\n",
    "This just plots 100 different CDFs, but it is a good way to understand what the eventual 95% CI plot is getting its data from!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26f0095a17aff466"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "NUM_SUBSETS = 100\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "for std in target_standards:\n",
    "    std_df = standards_df_dict[test_lib].query(\"assignment == @std\")\n",
    "    for size in [5, 10, 25, 50]:\n",
    "        for i in range(NUM_SUBSETS):\n",
    "            subset = std_df.sample(size)\n",
    "            sea.ecdfplot(data=subset, x='polya_length', ax=ax, label=f\"{std} - {size} reads\", color=colors[std], alpha=0.1)\n",
    "ax.set_title(\"CDF of tail lengths for different standards and read counts\")\n",
    "ax.set_xlabel(\"Tail length\")\n",
    "ax.set_ylabel(\"Proportion of reads\")\n",
    "# Can we change the limits so we just stop a x = 200:\n",
    "ax.set_xlim(0, 150)\n",
    "plt.savefig(f\"figures/tailLengthReproducibility.{test_lib}.{NUM_SUBSETS}subsets.CDFs.allLines.png\")\n",
    "plt.savefig(f\"figures/tailLengthReproducibility.{test_lib}.{NUM_SUBSETS}subsets.CDFs.allLines.svg\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.668221160Z"
    }
   },
   "id": "1d66e26166966459",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function to plot the mean CDF and 95% CI for each standard and subset size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d49afb28922ce555"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_cdf(tester_df__, ax__=None, sample_size=25, repeats=1000,\n",
    "             stds_color_map={'10': 'red', '15': 'green', '60': 'blue'},\n",
    "             x_axis_lim=200, log_x=False):\n",
    "    if ax__ is None:\n",
    "        fig, ax__ = plt.subplots(figsize=(7, 5))\n",
    "    \n",
    "    for std, color in stds_color_map.items():\n",
    "        std_df = tester_df__.query(\"assignment == @std\")\n",
    "\n",
    "        x_values = []\n",
    "        y_values = []\n",
    "\n",
    "        # Define the bin edges to span the range of 'polya_length_floored' across all subsets\n",
    "        bin_edges = np.linspace(std_df['polya_length_floored'].min(), std_df['polya_length_floored'].max(), num=10_000)\n",
    "\n",
    "        for _ in range(repeats):\n",
    "            try:\n",
    "                sample_df = std_df.sample(sample_size)\n",
    "            except ValueError as e:\n",
    "                if sample_size > std_df.shape[0]:\n",
    "                    print(f\"Sample size too large for std {std}! To resolve this we will sample with replacement!\")\n",
    "                    sample_df = std_df.sample(sample_size, replace=True)\n",
    "                else:\n",
    "                    raise e\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred with std {std} and sample size {sample_size}!\")\n",
    "                raise e\n",
    "            # Use the defined bin edges to calculate the histogram\n",
    "            hist, _ = np.histogram(sample_df['polya_length_floored'], bins=bin_edges)\n",
    "            # Calculate the cumulative sum to get the CDF\n",
    "            cdf = np.cumsum(hist)\n",
    "            # Normalize the CDF by the total count\n",
    "            cdf = cdf / sample_df.shape[0]\n",
    "            x_values.append(bin_edges[:-1])  # Exclude the last bin edge because np.histogram returns one more bin edge than the number of bins\n",
    "            y_values.append(cdf)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        x_values = np.array(x_values)\n",
    "        y_values = np.array(y_values)\n",
    "\n",
    "        # Calculate mean and 95% confidence interval of y values\n",
    "        mean_y = np.mean(y_values, axis=0)\n",
    "        lower_bound, upper_bound = np.percentile(y_values, [2.5, 97.5], axis=0)\n",
    "\n",
    "        # The x values should be the same for all repeats, so just take the x values from the first repeat\n",
    "        assert (x_values[0] == x_values[1]).all()\n",
    "        x = x_values[0]\n",
    "        ax__.plot(x, mean_y, color=color, label=f\"Mean CDF - {std}A\")\n",
    "        ax__.fill_between(x, lower_bound, upper_bound, color=color, alpha=0.1, label=f\"95% CI - {std}A\")\n",
    "        ax__.set_title(f\"Mean CDF and 95% CI for {repeats} repeats of {sample_size} reads\")\n",
    "    ax__.set_xlabel(\"Tail length\")\n",
    "    ax__.set_ylabel(\"Proportion of reads\")\n",
    "    ax__.legend()\n",
    "    \n",
    "    if log_x:\n",
    "        ax__.set_xlim(5, x_axis_lim)\n",
    "        ax__.set_xscale('log')\n",
    "        ax__.grid(True, which=\"both\", alpha=0.5)\n",
    "    else:\n",
    "        ax__.set_xlim(0, x_axis_lim)\n",
    "        ax__.grid(True, which=\"both\", alpha=0.5)\n",
    "    \n",
    "    if ax__ is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return ax__\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "_ = plot_cdf(standards_df_dict[test_lib], ax__=ax, log_x=True, sample_size=50, repeats=100)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.669376627Z"
    }
   },
   "id": "fffd8d7ed3bb6b3e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting the 95% CI CDFs for multiple sample sizes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d61235fa3fc839f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_cdfs(tester_df, sample_sizes, repeats=100, stds_color_map={'10': 'red', '15': 'green', '60': 'blue'}):\n",
    "    fig, axs = plt.subplots(len(sample_sizes), 1, figsize=(5, 5 * len(sample_sizes)))\n",
    "    for ax, sample_size in zip(axs, sample_sizes):\n",
    "        plot_cdf(tester_df, sample_size=sample_size, ax__=ax, repeats=repeats, stds_color_map=stds_color_map)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/tailLengthReproducibility.{test_lib}.{repeats}repeats.multiSampleSizes.CDFs.95CI.png\")\n",
    "    plt.savefig(f\"figures/tailLengthReproducibility.{test_lib}.{repeats}repeats.multiSampleSizes.CDFs.95CI.svg\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_cdfs_multi_col(tester_df, sample_sizes, repeats=100,\n",
    "                        stds_color_map={'10': 'red', '15': 'green', '60': 'blue'},\n",
    "                        col_width=3, col_num=2, row_height=5, log_x=False):\n",
    "    row_num = (len(sample_sizes) + col_num - 1) // col_num  # Calculate the number of rows\n",
    "    fig, axs = plt.subplots(row_num, col_num, figsize=(col_width * col_num, row_height * row_num))  # Create a grid of subplots\n",
    "    axs = axs.flatten()  # Flatten the array of axes\n",
    "    for ax, sample_size in zip(axs, sample_sizes):\n",
    "        plot_cdf(tester_df, sample_size=sample_size, ax__=ax, repeats=repeats, stds_color_map=stds_color_map, log_x=log_x)\n",
    "    plt.tight_layout()\n",
    "    save_path = f\"figures/tailLengthReproducibility.{test_lib}.{repeats}repeats.multiSampleSizes.CDFs.95CI\"\n",
    "    if log_x:\n",
    "        save_path += \".log\"\n",
    "    plt.savefig(save_path + \".png\")\n",
    "    plt.savefig(save_path + \".svg\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with a list of sample sizes\n",
    "plot_cdfs_multi_col(standards_df_dict[test_lib], [5, 10, 15, 25, 50, 100],\n",
    "                    repeats=100, col_num=3, col_width=5, row_height=5,\n",
    "                    log_x=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.670520584Z"
    }
   },
   "id": "b462ebb2a35deca9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <u>Reproducibility of Abundance</u> (Target II)\n",
    "\n",
    "## First, data loading:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e90e87cc7c5acce1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "read_df_dict = {}\n",
    "gene_df_dict = {}\n",
    "drop_standards = True\n",
    "\n",
    "for lib, obj in obj_dict.items():\n",
    "    print(f\"Processing {lib}...\")\n",
    "    read_df = obj.mergedOnReads_df.copy()\n",
    "    if drop_standards and lib != 'oldN2':\n",
    "        read_df = read_df.query(\"assignment == 'NotAStandard'\")\n",
    "    read_df.qc_pass_featc = read_df.qc_pass_featc.fillna(False)\n",
    "    read_df.qc_pass_featc = read_df.qc_pass_featc.astype(bool)\n",
    "    cols_to_keep = ['read_id', 'chr_id', 'chr_pos', 'qc_pass_featc', 'gene_id', 'gene_name', 'sequence', 'cigar', 'strand', 'read_length', 'polya_length', 'qc_tag_polya']\n",
    "    if lib != 'oldN2':\n",
    "        cols_to_keep += ['assignment']\n",
    "    read_df_dict[lib] = read_df[cols_to_keep]\n",
    "    print(read_df.value_counts('qc_pass_featc', normalize=True))\n",
    "    gene_df = obj.load_compressedOnGenes()  # Looks like the old N2 library had a read cutoff of 5 while everything else had no cutoff!!\n",
    "    if drop_standards:\n",
    "        gene_df = gene_df.query(\"gene_id != 'cerENO2'\")\n",
    "    gene_df_dict[lib] = gene_df\n",
    "\n",
    "read_hits_series_dict = {}\n",
    "for lib, df in gene_df_dict.items():\n",
    "    print(f\"Pre-cutdown:  {lib} - {df.shape[0]:,} Genes\", end=\" \")\n",
    "    # # TODO: Eventually, I should rerun the compressing for oldN2 without the cutoff!!!\n",
    "    df = df.query(\"read_hits >= 2\")\n",
    "    print(f\"Post-cutdown: {lib} - {df.shape[0]:,} Genes\")\n",
    "    hits_series = df[['gene_id', 'read_hits']].set_index('gene_id')\n",
    "    hits_series.rename(columns={'read_hits': lib}, inplace=True)\n",
    "    read_hits_series_dict[lib] = hits_series"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.671660031Z"
    }
   },
   "id": "94510044f9b0384b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_libs = [\n",
    "    \"oldN2\",\n",
    "    \"newerN2\",\n",
    "    \"newerS6\",\n",
    "    \"newerS5\",\n",
    "    \"thirdN2\",\n",
    "    \"thirdS5\",\n",
    "    \"thirdS6\",\n",
    "]\n",
    "\n",
    "plot_read_hits_table = pd.concat({lib: read_hits_series_dict[lib] for lib in plot_libs}.values(), axis=1).fillna(0)\n",
    "plot_read_hits_table['avg'] = plot_read_hits_table.mean(axis=1)\n",
    "plot_read_hits_table['avg_rounded'] = plot_read_hits_table['avg'].round(1)\n",
    "plot_read_hits_table['std'] = plot_read_hits_table.std(axis=1)\n",
    "plot_read_hits_table['std/avg'] = plot_read_hits_table['std'] / plot_read_hits_table['avg']\n",
    "if 'gene_name' not in plot_read_hits_table.columns:\n",
    "    gene_id_gene_name_df = npCommon.gene_names_to_gene_ids()\n",
    "    plot_read_hits_table.reset_index(names='gene_id', inplace=True)\n",
    "    plot_read_hits_table = plot_read_hits_table.merge(gene_id_gene_name_df, on='gene_id', how='left')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.672813288Z"
    }
   },
   "id": "31e0685df79ec841",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig = px.scatter(plot_read_hits_table.query(\"avg_rounded >= 5\"),\n",
    "                 x='avg_rounded', y='std/avg',\n",
    "                 hover_name='gene_name',\n",
    "                 log_x=True,\n",
    "                 opacity=0.5, )\n",
    "fig.update_layout(title=f\"<b>Reproducibility of gene abundance at various read count cutoffs</b><br>(Libraries: {', '.join(plot_libs)})\",\n",
    "                  xaxis_title=\"Average reads per gene\",\n",
    "                  yaxis_title=\"Standard deviation / Average reads per gene\",\n",
    "                  showlegend=False,\n",
    "                  width=800, height=800)\n",
    "fig.show()\n",
    "plot_read_hits_table"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.673971085Z"
    }
   },
   "id": "82b93737018ecf7d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def std_avg_plot(read_hits_series_dict, plot_libs, min_reads=10, filter_average=False, ax=None, gene_id_gene_name_df=None):\n",
    "    hits_table = pd.concat({lib: read_hits_series_dict[lib] for lib in plot_libs}.values(), axis=1).fillna(0)\n",
    "    hits_table['avg'] = hits_table.mean(axis=1)\n",
    "    hits_table['avg_rounded'] = hits_table['avg'].round(1)\n",
    "    hits_table['std'] = hits_table.std(axis=1)\n",
    "    hits_table['std/avg'] = hits_table['std'] / hits_table['avg']\n",
    "    if 'gene_name' not in hits_table.columns and gene_id_gene_name_df is not None:\n",
    "        hits_table.reset_index(names='gene_id', inplace=True)\n",
    "        hits_table = hits_table.merge(gene_id_gene_name_df, on='gene_id', how='left')\n",
    "    min_reads = min_reads\n",
    "    # Filter and sort the data\n",
    "    if filter_average:\n",
    "        data = hits_table.query(\"avg >= @min_reads\").sort_values('avg_rounded')\n",
    "    else:\n",
    "        # build query string\n",
    "        query = \" & \".join([f\"{lib} >= @min_reads\" for lib in plot_libs])\n",
    "        data = hits_table.query(query).sort_values('avg_rounded')\n",
    "    # Calculate the rolling mean and standard deviation\n",
    "    window_size = 50  # Adjust this value as needed\n",
    "    data['rolling_mean'] = data['std/avg'].rolling(window_size).mean()\n",
    "    data['rolling_std'] = data['std/avg'].rolling(window_size).std()\n",
    "    # Calculate the SEM\n",
    "    data['sem'] = data['rolling_std'] / np.sqrt(window_size)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.scatter(data['avg'], data['std/avg'], label='Data', alpha=0.1, color='black',\n",
    "                # s=5,\n",
    "                marker='.')\n",
    "    # Plot the rolling mean and SEM as a shaded region\n",
    "    ax.plot(data['avg_rounded'], data['rolling_mean'], color='red', label=f'Rolling Mean\\nWindow={window_size}')\n",
    "    ax.fill_between(data['avg_rounded'], data['rolling_mean'] - data['sem'], data['rolling_mean'] + data['sem'],\n",
    "                     color='red', alpha=0.4, label='SEM')\n",
    "    # Set the plot title and labels\n",
    "    ax.set_title(f\"Reproducibility of gene abundance at various read counts\\n(Libraries: {', '.join(plot_libs)})\")\n",
    "    ax.set_xlabel(\"Average reads per gene\")\n",
    "    # plt.xlabel(\"Lowest reads per gene\")\n",
    "    ax.set_ylabel(\"Standard deviation / Average reads per gene\")\n",
    "    ax.axvline(100, color='red', linestyle='--', label=\"Enforced cutoff\")\n",
    "    ax.legend()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(-0.05, 1)\n",
    "    ax.set_xlim(min_reads-1, 1e4)\n",
    "    ax.grid(True, which=\"both\", alpha=0.5)\n",
    "    if ax is None:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"figures/geneAbundanceReproducibility.{'-'.join(plot_libs)}.rollingMean{window_size}.SEM.png\")\n",
    "        plt.savefig(f\"figures/geneAbundanceReproducibility.{'-'.join(plot_libs)}.rollingMean{window_size}.SEM.svg\")\n",
    "        plt.show()\n",
    "        return None, hits_table\n",
    "    else:\n",
    "        return ax, hits_table\n",
    "    \n",
    "plot_libs = [\n",
    "    \"oldN2\",\n",
    "    \"newerN2\",\n",
    "    \"newerS6\",\n",
    "    \"newerS5\",\n",
    "    \"thirdN2\",\n",
    "    \"thirdS5\",\n",
    "    \"thirdS6\",\n",
    "]\n",
    "\n",
    "std_avg_plot(read_hits_series_dict, plot_libs, min_reads=10, filter_average=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.674936857Z"
    }
   },
   "id": "e520ab0701a64138",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "name_map = {\n",
    "    \"oldN2\": \"Wildtype (rep1)\",\n",
    "    \"oldS6\": \"<i>smg-6</i> (rep1)\",\n",
    "    \"newerN2\": \"Wildtype (rep2)\",\n",
    "    \"newerS6\": \"<i>smg-6</i> (rep2)\",\n",
    "    \"newerS5\": \"<i>smg-5</i> (rep2)\",\n",
    "    \"thirdN2\": \"Wildtype (rep3)\",\n",
    "    \"thirdS5\": \"<i>smg-5</i> (rep3)\",\n",
    "    \"thirdS6\": \"<i>smg-6</i> (rep3)\",\n",
    "}\n",
    "scatter_combos = [\n",
    "    # (\"oldN2\", \"newerN2\"),\n",
    "    # (\"oldN2\", \"thirdN2\"),\n",
    "    (\"newerN2\", \"thirdN2\"),\n",
    "    (\"newerS6\", \"thirdS6\"),\n",
    "    (\"newerS5\", \"thirdS5\"),\n",
    "]\n",
    "# for x_lib, y_lib in scatter_combos:\n",
    "#     try:\n",
    "#         data = plot_read_hits_table.query(f\"{x_lib} >= @min_reads & {y_lib} >= @min_reads\")\n",
    "#         x = data[x_lib]\n",
    "#         y = data[y_lib]\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: `{e}`, giving up on the plot for: {x_lib} vs {y_lib}!\")\n",
    "#         continue\n",
    "#     fig = px.scatter(x=x, y=y, hover_name=data['gene_name'],\n",
    "#                      log_x=True, log_y=True, opacity=0.5)\n",
    "#     fig.update_traces(marker=dict(size=5,\n",
    "#                                   color='black'))\n",
    "#     # I want to add a line at 100 reads per gene on the y and x axes:\n",
    "#     fig.add_shape(type=\"line\",\n",
    "#                   x0=100, y0=0, x1=100, y1=1,\n",
    "#                   line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
    "#                   xref=\"x\", yref=\"paper\")\n",
    "#     fig.add_shape(type=\"line\",\n",
    "#                     x0=0, y0=100, x1=1, y1=100,\n",
    "#                     line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
    "#                   xref=\"paper\", yref=\"y\")\n",
    "#     fig.update_layout(title=f\"<b>Reproducibility of gene abundance at various read count cutoffs</b><br>{name_map[x_lib]} vs {name_map[y_lib]}\",\n",
    "#                       xaxis_title=f\"Reads per gene - {name_map[x_lib]}\",\n",
    "#                       yaxis_title=f\"Reads per gene - {name_map[y_lib]}\",\n",
    "#                       showlegend=False,\n",
    "#                       width=800, height=800)\n",
    "#     fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.716766090Z"
    }
   },
   "id": "790446863b189617",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def rocket_plot(x_lib, y_lib, plot_read_hits_table, ax=None):\n",
    "    name_map = {\n",
    "    \"oldN2\": \"Wildtype (rep1)\",\n",
    "    \"oldS6\": \"smg-6 (rep1)\",\n",
    "    \"newerN2\": \"Wildtype (rep2)\",\n",
    "    \"newerS6\": \"smg-6 (rep2)\",\n",
    "    \"newerS5\": \"smg-5 (rep2)\",\n",
    "    \"thirdN2\": \"Wildtype (rep3)\",\n",
    "    \"thirdS5\": \"smg-5 (rep3)\",\n",
    "    \"thirdS6\": \"smg-6 (rep3)\",}\n",
    "    \n",
    "    try:\n",
    "        data = plot_read_hits_table.query(f\"{x_lib} >= @min_reads & {y_lib} >= @min_reads\")\n",
    "        x = data[x_lib]\n",
    "        y = data[y_lib]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: `{e}`, giving up on the plot for: {x_lib} vs {y_lib}!\")\n",
    "        return\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        \n",
    "    ax.scatter(x=x, y=y, alpha=0.5, color='black', marker='.')\n",
    "\n",
    "    ax.axvline(100, color='red', linestyle='--')\n",
    "    ax.axhline(100, color='red', linestyle='--')\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"Reproducibility of gene abundance at various read count cutoffs\\n{name_map[x_lib]} vs {name_map[y_lib]}\")\n",
    "    ax.set_xlabel(f\"Log10 Reads per gene - {name_map[x_lib]}\")\n",
    "    ax.set_ylabel(f\"Log10 Reads per gene - {name_map[y_lib]}\")\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "    ax.grid(True, which=\"both\", alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return ax\n",
    "\n",
    "for x_lib, y_lib in scatter_combos:\n",
    "    rocket_plot(x_lib, y_lib, plot_read_hits_table)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.716932535Z"
    }
   },
   "id": "8140d207b911cfb5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for libs in scatter_combos:\n",
    "    x_lib, y_lib = libs\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1, hits_df = std_avg_plot(read_hits_series_dict, [x_lib, y_lib], min_reads=10, filter_average=True, ax=ax1)\n",
    "    rocket_plot(x_lib, y_lib, hits_df, ax=ax2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/geneAbundanceReproducibility.{x_lib}.vs.{y_lib}.combined.png\")\n",
    "    plt.savefig(f\"figures/geneAbundanceReproducibility.{x_lib}.vs.{y_lib}.combined.svg\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.717004388Z"
    }
   },
   "id": "683ea42408045089",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <u>Saturation analysis</u> (Target III) (Part 2)\n",
    "Josh thinks we could additionally have a plot with: X-axis = reads_per_gene cutoff (1 to 200?) Y-axis = cumulative number of genes above X cutoff"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf34e8b9dc631d88"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_libs = libs_to_run\n",
    "plot_read_hits_table = pd.concat({lib: read_hits_series_dict[lib] for lib in plot_libs}.values(), axis=1).fillna(0)\n",
    "print(plot_read_hits_table.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.717068190Z"
    }
   },
   "id": "d03a90dc4db11e15",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the range of cutoffs\n",
    "cutoffs = range(1, 10001)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Initialize a dict to store the number of genes for each cutoff\n",
    "num_genes_dict = {}\n",
    "for lib in plot_libs:\n",
    "    num_genes = []\n",
    "    # For each cutoff, filter the data and count the number of genes\n",
    "    for cutoff in cutoffs:\n",
    "        filtered_data = plot_read_hits_table[lib][plot_read_hits_table[lib] >= cutoff]\n",
    "        num_genes.append(filtered_data.count())\n",
    "    ax.plot(cutoffs, num_genes, marker='.', label=lib)\n",
    "    num_genes_dict[lib] = num_genes\n",
    "\n",
    "# Plot the number of genes against the cutoff\n",
    "ax.set_xlabel('Reads per gene cutoff')\n",
    "ax.set_ylabel('Number of genes')\n",
    "ax.set_title('Saturation analysis')\n",
    "ax.grid(True)\n",
    "\n",
    "# Invert the x-axis\n",
    "ax.invert_xaxis()\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.grid(True, which=\"both\", alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/saturationAnalysis.geneCountsCutoffs.{'-'.join(plot_libs)}.png\")\n",
    "plt.savefig(f\"figures/saturationAnalysis.geneCountsCutoffs.{'-'.join(plot_libs)}.svg\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.717130212Z"
    }
   },
   "id": "751cb00e3721c6d2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-18T19:10:24.717196624Z"
    }
   },
   "id": "cd64a8dcf38c6cf3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
